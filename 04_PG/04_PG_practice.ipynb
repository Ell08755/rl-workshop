{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.special import softmax \n",
    "import time \n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from utils.env import frozen_lake, cart_pole\n",
    "from utils.viz import viz \n",
    "viz.get_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lr_curves(model_performances, window_size=40, step_unit=100, x_range=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 2.5))\n",
    "    for lbl, curve in model_performances.items():\n",
    "        # smooth the learning curve \n",
    "        smoothed_curve = np.convolve(curve, np.ones(window_size)/window_size, mode='valid')\n",
    "        t = np.arange(len(smoothed_curve))\n",
    "        sns.lineplot(x=t, y=smoothed_curve, ax=ax, label=lbl)\n",
    "    for pos in ['left', 'bottom']: ax.spines[pos].set_linewidth(3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    #ax.set_ylim([0, 105])\n",
    "    if x_range is not None: ax.set_xlim(x_range)\n",
    "    ax.set_xlabel(f'Every {step_unit} steps')\n",
    "    ax.set_ylabel('Reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234 \n",
    "env = frozen_lake(seed=seed, Rscale=10)\n",
    "env.reset()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "env.render(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 介绍一下这个frozen_lake的游戏\n",
    "* env.reset() 重置agent的位置到起始点\n",
    "* env.step(a) a=[0, 1, 2, 3,] 代表上下左右\n",
    "* env.p_s_next(s, a) 输入当前state s 和 action a, 输出转移到所有其他state的概率的向量\n",
    "* env.r(s) 到达某个state时，返回(reward, bool done or not)\n",
    "* env.s_termination 返回所有的终点的state, 包括窟窿和终点\n",
    "* env.show_v(ax, V)  把所有状态的value画出来\n",
    "* env.A, env.S 展示所有的action和state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_curves = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_policy:\n",
    "    \n",
    "    def __init__(self, nS, nA):\n",
    "        self.nS, self.nA = nS, nA\n",
    "        self.theta = np.zeros([nS, nA])\n",
    "\n",
    "    def forward(self, s):\n",
    "        return softmax(self.theta[s])\n",
    "    \n",
    "    def log_grad(self, s, a):\n",
    "        ''' log gradient of π(a|s) \n",
    "            for action==a:  1-π(a|s)\n",
    "            for action!=a:   -π(a|s)\n",
    "        '''\n",
    "        return np.eye(self.nA)[a] - self.forward(s)\n",
    "    \n",
    "    def get_pi(self):\n",
    "        return softmax(self.theta, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE(env, policy_strategy, alpha=.1, gamma=.99, \n",
    "                  seed=1234, \n",
    "                  show_trajectory=False, \n",
    "                  show_intval=1000, max_steps=100,\n",
    "                  step_unit=100, max_step_unit=4000):\n",
    "    \n",
    "    # initialize random seed\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # init a list to record the model performance per episode\n",
    "    model_performance = []\n",
    "    # initialize the policy strategy\n",
    "    policy = policy_strategy(env.nS, env.nA)\n",
    "\n",
    "    # loop over episodes \n",
    "    epi, end_learn = 0, False\n",
    "    step_cum, r_cum, n_step_unit = 0, 0, 0\n",
    "    while not end_learn:\n",
    "        trajectory = []\n",
    "\n",
    "        # rollout the episode \n",
    "        s, _, _ = env.reset()\n",
    "        step = 0\n",
    "        while True:\n",
    "            ######################################\n",
    "            # TODO: rollout the episode\n",
    "            #       * sample an action\n",
    "            #       * interact with the environment\n",
    "            #       * record the trajectory\n",
    "            ######################################\n",
    "            s = s_next\n",
    "\n",
    "            # for evaluation  \n",
    "            step += 1\n",
    "            r_cum += r\n",
    "            step_cum += 1\n",
    "            \n",
    "            if (step_cum+1) % step_unit==0:\n",
    "                model_performance.append(r_cum)\n",
    "                step_cum, r_cum = 0, 0\n",
    "                n_step_unit += 1\n",
    "\n",
    "            if n_step_unit >= max_step_unit:\n",
    "                end_learn = True\n",
    "                break\n",
    "           \n",
    "            if show_trajectory and (epi%show_intval==0):\n",
    "                fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                clear_output(True)\n",
    "                env.render(ax[0], epi+1, step)\n",
    "                env.show_pi(ax[1], policy.get_pi())\n",
    "                time.sleep(.1)\n",
    "                plt.show()\n",
    "\n",
    "            if done or (step>max_steps): \n",
    "                epi += 1\n",
    "                break\n",
    "        \n",
    "        # update the policy \n",
    "        T = len(trajectory)\n",
    "        for t, expr in enumerate(trajectory):\n",
    "            s, a, r = expr\n",
    "            ######################################\n",
    "            # TODO: calculate the return G\n",
    "            ######################################\n",
    "            ######################################\n",
    "            # TODO: update the policy parameter\n",
    "            ######################################\n",
    "\n",
    "    return policy.get_pi(), model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the random policy\n",
    "seed = 1234\n",
    "for alpha in [.01, .05, .75, .1, .2]: \n",
    "    env = frozen_lake(seed=seed, Rscale=10)\n",
    "    pi_re, mp_re = REINFORCE(env, policy_strategy=softmax_policy, \n",
    "                            alpha=alpha,\n",
    "                            seed=seed+1,\n",
    "                            show_trajectory=False,\n",
    "                            max_step_unit=3000)\n",
    "    lr_curves[f'reinforce:a={alpha}'] = mp_re\n",
    "show_lr_curves(lr_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove the variants other than the best one\n",
    "for alpha in [.01, .05, .75, .2]: del lr_curves[f'reinforce:a={alpha}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actor-Critic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classical Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actor_Critic(env, policy_strategy, \n",
    "                  alpha_v=.1, alpha_theta=.1, gamma=.99, \n",
    "                  seed=1234, \n",
    "                  show_trajectory=False, \n",
    "                  show_intval=1000, max_steps=100,\n",
    "                  step_unit=100, max_step_unit=4000):\n",
    "    \n",
    "    # initialize random seed\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # init a list to record the model performance per episode\n",
    "    model_performance = []\n",
    "    # initialize the policy strategy\n",
    "    actor = policy_strategy(env.nS, env.nA)\n",
    "    v_critic = np.zeros([env.nS])\n",
    "\n",
    "    # loop over episodes \n",
    "    epi, end_learn = 0, False\n",
    "    step_cum, r_cum, n_step_unit = 0, 0, 0\n",
    "    while not end_learn:\n",
    "\n",
    "        # rollout the episode \n",
    "        s, _, _ = env.reset()\n",
    "        step = 0\n",
    "        I = 1 \n",
    "        while True:\n",
    "            \n",
    "            ######################################\n",
    "            # TODO: rollout the episode\n",
    "            #       * sample an action\n",
    "            #       * interact with the environment\n",
    "            ######################################\n",
    "\n",
    "            # update the agent \n",
    "            ######################################\n",
    "            # TODO: calculate the update target \n",
    "            #         and prediction error\n",
    "            ######################################\n",
    "            ######################################\n",
    "            # TODO: update the critic\n",
    "            ######################################\n",
    "            ######################################\n",
    "            # TODO: update the actor with Q\n",
    "            ######################################\n",
    "            s = s_next\n",
    "\n",
    "            # for evaluation\n",
    "            step += 1\n",
    "            r_cum += r\n",
    "            step_cum += 1\n",
    "            \n",
    "            if (step_cum+1) % step_unit==0:\n",
    "                model_performance.append(r_cum)\n",
    "                step_cum, r_cum = 0, 0\n",
    "                n_step_unit += 1\n",
    "\n",
    "            if n_step_unit >= max_step_unit:\n",
    "                end_learn = True\n",
    "                break\n",
    "           \n",
    "            if show_trajectory and (epi%show_intval==0):\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                clear_output(True)\n",
    "                env.render(ax[0], epi+1, step)\n",
    "                env.show_v(ax[1], v_critic)\n",
    "                env.show_pi(ax[2], actor.get_pi())\n",
    "                time.sleep(.1)\n",
    "                plt.show()\n",
    "\n",
    "            if done or (step>max_steps): \n",
    "                epi += 1\n",
    "                break\n",
    "        \n",
    "    return actor.get_pi(), model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the random policy\n",
    "seed = 1234\n",
    "env = frozen_lake(seed=seed, Rscale=10)\n",
    "alpha_v, alpha_theta = .1, .2\n",
    "pi, mp = Actor_Critic(env, policy_strategy=softmax_policy, \n",
    "                        alpha_v=alpha_v,\n",
    "                        alpha_theta=alpha_theta,\n",
    "                        seed=seed+1,\n",
    "                        show_trajectory=False,\n",
    "                        max_step_unit=3000)\n",
    "lr_curves[f'actor-critic'] = mp\n",
    "show_lr_curves(lr_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adv_Actor_Critic(env, policy_strategy, \n",
    "                  alpha_v=.1, alpha_theta=.1, gamma=.99, \n",
    "                  seed=1234, \n",
    "                  show_trajectory=False, \n",
    "                  show_intval=1000, max_steps=100,\n",
    "                  step_unit=100, max_step_unit=4000):\n",
    "    \n",
    "    # initialize random seed\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # init a list to record the model performance per episode\n",
    "    model_performance = []\n",
    "    # initialize the policy strategy\n",
    "    actor = policy_strategy(env.nS, env.nA)\n",
    "    v_critic = np.zeros([env.nS])\n",
    "\n",
    "    # loop over episodes \n",
    "    epi, end_learn = 0, False\n",
    "    step_cum, r_cum, n_step_unit = 0, 0, 0\n",
    "    while not end_learn:\n",
    "\n",
    "        # rollout the episode \n",
    "        s, _, _ = env.reset()\n",
    "        step = 0\n",
    "        I = 1 \n",
    "        while True:\n",
    "            ######################################\n",
    "            # TODO: rollout the episode\n",
    "            #       * sample an action\n",
    "            #       * interact with the environment\n",
    "            ######################################\n",
    "\n",
    "            # update the agent \n",
    "            ######################################\n",
    "            # TODO: calculate the update target \n",
    "            #         and prediction error\n",
    "            ######################################\n",
    "            ######################################\n",
    "            # TODO: update the critic\n",
    "            ######################################\n",
    "            ######################################\n",
    "            # TODO: update the actor with A (advantage)\n",
    "            ######################################\n",
    "            s = s_next\n",
    "\n",
    "            # for evaluation\n",
    "            step += 1\n",
    "            r_cum += r\n",
    "            step_cum += 1\n",
    "            \n",
    "            if (step_cum+1) % step_unit==0:\n",
    "                model_performance.append(r_cum)\n",
    "                step_cum, r_cum = 0, 0\n",
    "                n_step_unit += 1\n",
    "\n",
    "            if n_step_unit >= max_step_unit:\n",
    "                end_learn = True\n",
    "                break\n",
    "           \n",
    "            if show_trajectory and (epi%show_intval==0):\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                clear_output(True)\n",
    "                env.render(ax[0], epi+1, step)\n",
    "                env.show_v(ax[1], v_critic)\n",
    "                env.show_pi(ax[2], actor.get_pi())\n",
    "                time.sleep(.1)\n",
    "                plt.show()\n",
    "\n",
    "            if done or (step>max_steps): \n",
    "                epi += 1\n",
    "                break\n",
    "        \n",
    "    return actor.get_pi(), model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the random policy\n",
    "seed = 1234\n",
    "env = frozen_lake(seed=seed, Rscale=10)\n",
    "alpha_v, alpha_theta = .2, .2\n",
    "pi, mp = Adv_Actor_Critic(env, policy_strategy=softmax_policy, \n",
    "                        alpha_v=alpha_v,\n",
    "                        alpha_theta=alpha_theta,\n",
    "                        seed=seed+1,\n",
    "                        show_trajectory=False,\n",
    "                        show_intval=200,\n",
    "                        max_step_unit=3000)\n",
    "lr_curves[f'Adv actor-critic'] = mp\n",
    "show_lr_curves(lr_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, pi, policy_strategy, alpha=.1, gamma=.99, \n",
    "                  seed=1234, n_episodes=15000, \n",
    "                  show_trajectory=False, \n",
    "                  show_intval=1000, max_steps=100, \n",
    "                  step_unit=100, max_step_unit=4000):\n",
    "    \n",
    "    # initialization \n",
    "    rng = np.random.RandomState(seed)\n",
    "    q = np.zeros([env.nS, env.nA])\n",
    "    v = np.zeros([env.nS])\n",
    "    policy = policy_strategy(n_episodes)\n",
    "    model_performance = []\n",
    "\n",
    "    # loop over episodes \n",
    "    epi, end_learn = 0, False\n",
    "    step_cum, r_cum, n_step_unit = 0, 0, 0\n",
    "    while not end_learn:\n",
    "\n",
    "        policy.epi = epi\n",
    "        s, _, _ = env.reset()\n",
    "        a = rng.choice(env.A, p=pi[s])\n",
    "        step = 0\n",
    "        while True:\n",
    "            # interact with the env\n",
    "            s_next, r, done = env.step(a)\n",
    "            a_next = rng.choice(env.A, p=pi[s_next])\n",
    "            q_tar = r + gamma*(1-done)*q[s_next, a_next]\n",
    "            q[s, a] = q[s, a] + alpha*(q_tar - q[s, a])\n",
    "            # improve policy \n",
    "            pi[s] = policy(q[s])\n",
    "            # calculate the value function (just for visualization)\n",
    "            v[s] = np.sum(pi[s]*q[s])  \n",
    "            s, a = s_next, a_next\n",
    "            # for evaluation\n",
    "            step += 1\n",
    "            r_cum += r\n",
    "            step_cum += 1\n",
    "\n",
    "            if (step_cum+1) % step_unit==0:\n",
    "                model_performance.append(r_cum)\n",
    "                step_cum, r_cum = 0, 0\n",
    "                n_step_unit += 1\n",
    "\n",
    "            if n_step_unit >= max_step_unit:\n",
    "                end_learn = True\n",
    "                break\n",
    "            \n",
    "            if show_trajectory and (epi%show_intval==0):\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                clear_output(True)\n",
    "                env.render(ax[0], epi+1, step)\n",
    "                env.show_v(ax[1], v)\n",
    "                env.show_pi(ax[2], pi)\n",
    "                time.sleep(.1)\n",
    "                plt.show()\n",
    "\n",
    "            if done or (step>max_steps): \n",
    "                epi += 1\n",
    "                break\n",
    "\n",
    "    return pi, model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class e_greedy:\n",
    "    '''epsilon-greedy policy\n",
    "\n",
    "    The policy has epsilon probability to choose a random action,\n",
    "\n",
    "    for example\n",
    "        given q[s] = [.3, .9, .9, .1], the 2nd and 3rd action are tied,\n",
    "        then the greedy policy should be [0, .5, .5, 0]\n",
    "\n",
    "        assuming that epsilon = 0.1, then the policy should be \n",
    "\n",
    "        [0.025, 0.475, 0.475, 0.025]\n",
    "    '''\n",
    "    def __init__(self, n_episodes, eps=0.3):\n",
    "        self.n_episodes = n_episodes\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, q):\n",
    "        # random policy\n",
    "        pi_rand = np.ones(env.nA) / env.nA\n",
    "        # greedy policy \n",
    "        qmax = q==np.max(q)\n",
    "        pi_greedy = qmax / qmax.sum()\n",
    "        return self.eps*pi_rand + (1-self.eps)*pi_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234 \n",
    "rng = np.random.RandomState(seed)\n",
    "pi_rand = softmax(np.zeros([env.nS, env.nA]), axis=1)\n",
    "env = frozen_lake(seed=seed, Rscale=10)\n",
    "pi_sarsa, pg_sarsa = Sarsa(env, pi_rand, \n",
    "                            alpha=.08,\n",
    "                            policy_strategy=e_greedy, \n",
    "                            show_trajectory=False,\n",
    "                            max_step_unit=3000)\n",
    "lr_curves['sarsa'] = pg_sarsa\n",
    "show_lr_curves(lr_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous Action Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = cart_pole()\n",
    "s, _, done = env.reset()\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    \n",
    "    # Sample a random continuous action (force between -1 and 1)\n",
    "    action = np.random.uniform(-1.0, 1.0)\n",
    "    next_s, r, done = env.step(action)\n",
    "    total_reward += r\n",
    "    \n",
    "    # visualize\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    clear_output(True)\n",
    "    env.render(ax)\n",
    "    ax.set_title(f'Total Reward: {total_reward}')\n",
    "    plt.show()\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------  Optional Section  ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net_policy(nn.Module):\n",
    "\n",
    "    def __init__(self, nS, nA, alpha, gamma, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.Linear(nS, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, nA)\n",
    "        self.logsig = nn.Parameter(torch.zeros(nA))\n",
    "        # optimizer \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "    def forward(self, s):\n",
    "        # state -> action (mean of Gaussian)\n",
    "        x = torch.relu(self.l1(s))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        return torch.tanh(self.l3(x)), self.logsig\n",
    "    \n",
    "    def get_action(self, s):\n",
    "        s = torch.from_numpy(s).float().unsqueeze(0)\n",
    "        mu, logsig = self.forward(s)\n",
    "        dist = torch.distributions.Normal(mu, logsig.exp())\n",
    "        a = dist.sample()\n",
    "        log_prob = dist.log_prob(a)\n",
    "        return a.numpy()[0, 0], log_prob.sum()\n",
    "    \n",
    "    def update(self, log_probs, rs):\n",
    "        # compute returns \n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rs):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        # tensorize \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "        log_probs = torch.hstack(log_probs)\n",
    "        # compute loss \n",
    "        loss = -(returns * log_probs).mean()\n",
    "        # update \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_policy:\n",
    "\n",
    "    def __init__(self, nS, nA, alpha, gamma, hidden_dim=128):\n",
    "        pass\n",
    "\n",
    "    def get_action(self, s):\n",
    "        return np.random.uniform(-1.0, 1.0), 0\n",
    "    \n",
    "    def update(self, log_probs, rs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE(env, policy_strategy, alpha=.1, gamma=.99, \n",
    "                  seed=1234, \n",
    "                  show_trajectory=False, \n",
    "                  show_intval=200, max_steps=300,\n",
    "                  max_episodes=10000):\n",
    "    \n",
    "    # initialize random seed\n",
    "    torch.manual_seed(seed)    \n",
    "    # init a list to record the model performance per episode\n",
    "    model_performance = []\n",
    "    # initialize the policy strategy\n",
    "    policy = policy_strategy(env.nS, 1, alpha, gamma)\n",
    "\n",
    "    # loop over episodes \n",
    "    for epi in range(max_episodes):\n",
    "\n",
    "        # rollout the episode \n",
    "        s, _, _ = env.reset()\n",
    "        log_probs, rs = [], [] \n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            a, log_prob = policy.get_action(s)\n",
    "            s_next, r, done = env.step(a)\n",
    "            log_probs.append(log_prob)\n",
    "            rs.append(r)\n",
    "            s = s_next\n",
    "            step += 1\n",
    "            total_reward += r\n",
    "                       \n",
    "            if show_trajectory and (epi%show_intval==0):\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "                clear_output(True)\n",
    "                env.render(ax)\n",
    "                ax.set_title(f'Epi: {epi+1}, Reward: {total_reward}')\n",
    "                plt.show()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "            if done or (step>max_steps): \n",
    "                model_performance.append(total_reward)\n",
    "                break\n",
    "        \n",
    "        # update the policy \n",
    "        policy.update(log_probs, rs)\n",
    "\n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_curves = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 98\n",
    "env = cart_pole(seed)\n",
    "mp = REINFORCE(env, policy_strategy=neural_net_policy, \n",
    "                        alpha=.005,\n",
    "                        seed=seed+1,\n",
    "                        show_trajectory=False,\n",
    "                        max_episodes=2000)\n",
    "lr_curves['reinforce'] = mp\n",
    "show_lr_curves(lr_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 98\n",
    "np.random.seed(seed)\n",
    "env = cart_pole()\n",
    "mp = REINFORCE(env, policy_strategy=random_policy, \n",
    "                        alpha=.005,\n",
    "                        seed=seed+1,\n",
    "                        show_trajectory=False,\n",
    "                        max_episodes=2000)\n",
    "lr_curves['random'] = mp\n",
    "show_lr_curves(lr_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, nS, nA, alpha, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(nS, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, nA)\n",
    "        self.logsig = nn.Parameter(torch.zeros(nA))\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "    def forward(self, s):\n",
    "        x = torch.relu(self.l1(s))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        return torch.tanh(self.l3(x)), self.logsig\n",
    "\n",
    "    def get_action(self, s):\n",
    "        s = torch.from_numpy(s).float().unsqueeze(0)\n",
    "        mu, logsig = self.forward(s)\n",
    "        dist = torch.distributions.Normal(mu, logsig.exp())\n",
    "        a = dist.sample()\n",
    "        log_prob = dist.log_prob(a).sum()\n",
    "        return a.detach().numpy()[0, 0], log_prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, nS, alpha, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(nS, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "    def forward(self, s):\n",
    "        x = torch.relu(self.l1(s))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        return self.l3(x).squeeze()\n",
    "\n",
    "class actor_critic_policy:\n",
    "    def __init__(self, nS, nA, alpha_actor=1e-4, alpha_critic=1e-4, gamma=0.99, hidden_dim=128):\n",
    "        self.actor = Actor(nS, nA, alpha_actor, hidden_dim)\n",
    "        self.critic = Critic(nS, alpha_critic, hidden_dim)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_action(self, s):\n",
    "        action, log_prob = self.actor.get_action(s)\n",
    "        return action, log_prob\n",
    "\n",
    "    def update(self, log_prob, s, r, s_next, done):\n",
    "        s = torch.from_numpy(s).float().unsqueeze(0)\n",
    "        s_next = torch.from_numpy(s_next).float().unsqueeze(0)\n",
    "        v = self.critic.forward(s)\n",
    "        v_next = self.critic.forward(s_next)\n",
    "\n",
    "        # Compute TD targets and advantages \n",
    "        with torch.no_grad():\n",
    "            next_state_values = v_next * (1 - done)\n",
    "            q = r + self.gamma * next_state_values\n",
    "            advantage = q - v\n",
    "\n",
    "        # Actor loss\n",
    "        actor_loss = -(advantage * log_prob).mean()\n",
    "\n",
    "        # Critic loss\n",
    "        critic_loss = (q - v).square().mean()\n",
    "\n",
    "        # Update actor\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        # Update critic\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, policy_strategy, \n",
    "                  alpha_critic=.1, alpha_actor=.1,\n",
    "                  gamma=.99, \n",
    "                  seed=1234, \n",
    "                  show_trajectory=False, \n",
    "                  show_intval=200, max_steps=300,\n",
    "                  max_episodes=10000):\n",
    "    \n",
    "    # initialize random seed\n",
    "    torch.manual_seed(seed)    \n",
    "    # init a list to record the model performance per episode\n",
    "    model_performance = []\n",
    "    # initialize the policy strategy\n",
    "    ac_agent = policy_strategy(env.nS, 1, \n",
    "                               alpha_critic=alpha_critic, \n",
    "                               alpha_actor=alpha_actor, \n",
    "                               gamma=gamma)\n",
    "\n",
    "    # loop over episodes \n",
    "    for epi in range(max_episodes):\n",
    "\n",
    "        # rollout the episode \n",
    "        s, _, _ = env.reset()\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            a, log_prob = ac_agent.get_action(s)\n",
    "            s_next, r, done = env.step(a)\n",
    "            # update the policy \n",
    "            ac_agent.update(log_prob, s, r, s_next, done)\n",
    "\n",
    "            s = s_next\n",
    "            step += 1\n",
    "            total_reward += r\n",
    "                       \n",
    "            if show_trajectory and (epi%show_intval==0):\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "                clear_output(True)\n",
    "                env.render(ax)\n",
    "                ax.set_title(f'Epi: {epi+1}, Reward: {total_reward}')\n",
    "                plt.show()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "            if done or (step>max_steps): \n",
    "                model_performance.append(total_reward)\n",
    "                break\n",
    "        \n",
    "        \n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 98\n",
    "env = cart_pole(seed)\n",
    "mp = actor_critic(env, policy_strategy=actor_critic_policy, \n",
    "                        alpha_critic=1e-3,\n",
    "                        alpha_actor=1e-4,\n",
    "                        show_trajectory=False,\n",
    "                        max_episodes=2000)\n",
    "lr_curves['actor_critic'] = mp\n",
    "show_lr_curves(lr_curves)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
