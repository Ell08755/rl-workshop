{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.special import softmax \n",
    "import time \n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from utils.env import frozen_lake\n",
    "from utils.viz import viz \n",
    "viz.get_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234 \n",
    "env = frozen_lake(seed=seed)\n",
    "env.reset()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "env.render(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "acts = [3, 3, 3, 1, 1, 1]\n",
    "for a in acts:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    clear_output(True)\n",
    "    env.render(ax)\n",
    "    plt.show()\n",
    "    if done: break\n",
    "    _, _, done =env.step(a)\n",
    "    time.sleep(.1)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at the environment \n",
    "\n",
    "Actions: \n",
    "\n",
    "* 0: up\n",
    "* 1: down\n",
    "* 2: left\n",
    "* 3: right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check transition function\n",
    "# check p_trans of a surface\n",
    "env.p_s_next(s=1, a=2).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check p trans of a hole\n",
    "env.p_s_next(s=19, a=2).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check reward function of a surface, hole, and goal \n",
    "env.r(2), env.r(19), env.r(63), "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a random policy \n",
    "rng = np.random.RandomState(1234)\n",
    "pi_rand = softmax(rng.rand(env.nS, env.nA)*5, axis=1)\n",
    "pi_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(pi, V, env, theta=1e-4, gamma=.99):\n",
    "\n",
    "    # loop until convergence\n",
    "    while True: \n",
    "        delta = 0\n",
    "        for s in env.S:\n",
    "            if s not in env.s_termination:\n",
    "                v_old = V[s].copy()\n",
    "                v_new = 0\n",
    "                for a in env.A:\n",
    "                    p = env.p_s_next(s, a)\n",
    "                    for s_next in env.S:\n",
    "                        r, done = env.r(s_next)\n",
    "                        v_new += pi[s, a]*p[s_next]*(r + (1-done)*gamma*V[s_next])\n",
    "                V[s] = v_new \n",
    "                # check convergence\n",
    "                delta = np.max([delta, np.abs(v_new - v_old)])\n",
    "        \n",
    "        if delta < theta:\n",
    "            break \n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize V(s), arbitrarily except V(terminal)=0\n",
    "V = rng.rand(env.nS) * 0.001\n",
    "# except v(terminal) = 0\n",
    "for s in env.s_termination:\n",
    "    V[s] = 0\n",
    "v1 = policy_eval(pi_rand, V, env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(pi, V, env, theta=1e-4, gamma=.99):\n",
    "    pi_old = pi.copy()\n",
    "    for s in env.S:\n",
    "        q = np.zeros([env.nA])\n",
    "        for a in env.A:\n",
    "            p = env.p_s_next(s, a)\n",
    "            for s_next in env.S:\n",
    "                r, done = env.r(s_next)\n",
    "                q[a] += p[s_next]*(r + (1-done)*gamma*V[s_next])\n",
    "        pi[s] = np.eye(env.nA)[np.argmax(q)]\n",
    "    \n",
    "    # check stable\n",
    "    stable = (np.abs(pi - pi_old) < theta).all()\n",
    "\n",
    "    return pi, stable  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iter(env, seed=1234):\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    # initialize V(s), arbitrarily except V(terminal)=0\n",
    "    V = rng.rand(env.nS) * 0.001\n",
    "    # except v(terminal) = 0\n",
    "    for s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    # initialize Ï€(s), arbitrarily\n",
    "    pi = softmax(rng.rand(env.nS, env.nA)*5, axis=1)\n",
    "\n",
    "    while True: \n",
    "\n",
    "        V = policy_eval(pi, V, env)\n",
    "        pi, stable = policy_improve(pi, V, env)\n",
    "        if stable: break \n",
    "\n",
    "    return V, pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1, pi1 = policy_iter(env)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = axs[0]\n",
    "env.show_v(ax, V1)\n",
    "ax = axs[1]\n",
    "env.show_pi(ax, pi1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env, seed=1234, theta=1e-4, gamma=.99):\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    # initialize V(s), arbitrarily except V(terminal)=0\n",
    "    V = rng.rand(env.nS) * 0.001\n",
    "    # except v(terminal) = 0\n",
    "    for s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    # init policy \n",
    "    pi = np.zeros([env.nS, env.nA])\n",
    "    # loop until converge\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.S:\n",
    "            v_old = V[s].copy()\n",
    "            q = np.zeros([env.nA])\n",
    "            for a in env.A:\n",
    "                p = env.p_s_next(s, a)\n",
    "                for s_next in env.S:\n",
    "                    r, done = env.r(s_next)\n",
    "                    q[a] += p[s_next]*(r + (1-done)*gamma*V[s_next])\n",
    "            V[s] = np.max(q)\n",
    "            pi[s] = np.eye(env.nA)[np.argmax(q)]\n",
    "            delta = np.max([delta, np.abs(V[s] - v_old)])\n",
    "\n",
    "        if delta < theta:\n",
    "            break \n",
    "    for s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    return V, pi \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2, pi2 = value_iter(env)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = axs[0]\n",
    "env.show_v(ax, V2)\n",
    "ax = axs[1]\n",
    "env.show_pi(ax, pi2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD learning, Q learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(q, rng, env, eps):\n",
    "    a_max = np.argwhere(q==np.max(q)).flatten()\n",
    "    policy = np.sum([np.eye(env.nA)[i] for i in a_max], axis=0) / len(a_max)\n",
    "    if rng.rand() < 1-eps:\n",
    "        a = rng.choice(env.nA, p=policy)\n",
    "    else:\n",
    "        a = rng.choice(env.nA)\n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, alpha=.1, eps=.1, gamma=.99, max_epi=2000, seed=1234, theta=1e-4):\n",
    "    # rng\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # initialize Q\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    for epi in range(max_epi):\n",
    "        s, r, done = env.reset()\n",
    "        t = 0 \n",
    "        q_old = Q.copy()\n",
    "        G = 0\n",
    "        while True:\n",
    "            # sample At, observe Rt, St+1\n",
    "            a = e_greedy(Q[s, :], rng, env, eps)\n",
    "            # a = rng.choice(env.A, p=pi)\n",
    "            s_next, r, done = env.step(a)\n",
    "            Q_tar = r + gamma*(1-done)*(Q[s_next, :]).max()\n",
    "            Q[s, a] += alpha*(Q_tar - Q[s, a])\n",
    "            s = s_next \n",
    "            t += 1\n",
    "            G += r\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "\n",
    "            # if epi > 1400:\n",
    "            #     Pi = np.eye(env.nA)[np.argmax(Q, axis=1)]\n",
    "            #     V = Q.max(1)\n",
    "            #     fig, axs = plt.subplots(1, 3, figsize=(11, 4))\n",
    "            #     clear_output(True)\n",
    "            #     ax = axs[0]\n",
    "            #     env.render(ax)\n",
    "            #     ax = axs[1]\n",
    "            #     env.show_v(ax, V)\n",
    "            #     ax = axs[2]\n",
    "            #     env.show_pi(ax, Pi)\n",
    "            #     time.sleep(.05)\n",
    "            #     plt.show()\n",
    "            \n",
    "        if (np.abs(q_old - Q)<theta).all():\n",
    "            break\n",
    "    pi = np.eye(env.nA)[np.argmax(Q, axis=1)]\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, pi3 = Q_learning(env)\n",
    "V3 = Q.max(1)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = axs[0]\n",
    "env.show_v(ax, V3)\n",
    "ax = axs[1]\n",
    "env.show_pi(ax, pi3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
